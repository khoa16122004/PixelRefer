# Adopted from https://github.com/haotian-liu/LLaVA. Below is the original copyright:
# Adopted from https://github.com/lm-sys/FastChat. Below is the original copyright:
# Adopted from tatsu-lab@stanford_alpaca. Below is the original copyright:
#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li
#
#    Licensed under the Apache License, Version 2.0 (the "License");
#    you may not use this file except in compliance with the License.
#    You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.

import re
import os
import copy
import json
import random
import pathlib
import traceback
from dataclasses import dataclass, field
from typing import Dict, Optional, Sequence, List, Literal

# torch-related packages
# NOTE: torch must be imported before transformers. Otherwise, `Segmentation fault (core dumped)` will occur.
import torch
from torch.utils.data import Dataset

import transformers
from transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock

import sys
sys.path.append('./')
from videorefer.model import *
from videorefer.constants import NUM_FRAMES, IGNORE_INDEX, MODAL_INDEX_MAP, NUM_TIME_BINS
from videorefer.mm_utils import tokenizer_multimodal_token, process_video, process_image, annToMask
from videorefer.videorefer_trainer import (VideoReferTrainer,
    get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3, 
    find_all_linear_names, safe_save_model_for_hf_trainer
)
import numpy as np

from data_utils import timestampify_pt, timestamp_to_time_token

# NOTE: fast tokenizer warning issue: https://github.com/huggingface/transformers/issues/5486   
os.environ["TOKENIZERS_PARALLELISM"] = "true"

local_rank = None


def rank0_print(*args):
    if local_rank == 0:
        print(*args)


def set_seed(seed=42):
    """
    Set the random seed for reproducible results.

    :param seed: An integer value to be used as the random seed.
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU setups
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


@dataclass
class ModelArguments:
    # LLM Arguments
    model_type: Optional[str] = field(default="videorefer", metadata={"help": "Model type selected in the list: " + ", ".join(VLLMs.keys())})
    model_path: Optional[str] = field(default="lmsys/vicuna-7b-v1.5")
    version: Optional[str] = field(default="v1", metadata={"help": "Version of the conversation template."})
    freeze_backbone: bool = field(default=False, metadata={"help": "Whether to freeze the LLM backbone."})
    # Connector Arguments
    mm_projector_type: Optional[str] = field(default='linear')
    tune_mm_mlp_adapter: bool = field(default=False)
    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)
    # Region encoder Arguments
    mm_region_encoder_type: Optional[str] = field(default='pooling')
    tune_region_encoder: bool = field(default=False)
    pretrain_region_encoder: Optional[str] = field(default=None)
    # Vision tower Arguments
    vision_tower: Optional[str] = field(default=None)
    mm_vision_select_layer: Optional[int] = field(default=-1)
    mm_vision_select_feature: Optional[str] = field(default="patch")

@dataclass
class DataArguments:
    # Path Arguments
    data_path: str = field(default=None, metadata={"help": "Path to the training data."})
    # image_folder: Optional[str] = field(default=None)
    # video_folder: Optional[str] = field(default=None)
    data_folder: Optional[str] = field(default=None)
    # Loading Arguments
    is_multimodal: bool = False
    lazy_preprocess: bool = False
    num_frames: Optional[int] = field(default=None)
    num_mask_frames: Optional[int] = field(default=2) # 32
    region_token_num: Optional[int] = field(default=8) # 32
    # Preprocess Arguments
    image_aspect_ratio: str = 'square'
    llm_response_selector: Literal['GPT', 'Gemini', 'Qwen'] = field(default='Gemini', metadata={"help": "LLM response selector choice. Options: GPT, Gemini, or Qwen."})
    is_pretraining: bool = False


@dataclass
class TrainingArguments(transformers.TrainingArguments):
    optim: str = field(default="adamw_torch")
    mm_projector_lr: Optional[float] = None
    freeze_mm_mlp_adapter: bool = field(default=False)
    remove_unused_columns: bool = field(default=False)
    # Training Data Arguments 
    group_by_modality_length: bool = field(default=False)
    model_max_length: int = field(
        default=512,
        metadata={
            "help":
            "Maximum sequence length. Sequences will be right padded (and possibly truncated)."
        },
    )
    # Lora or Quant Arguments
    double_quant: bool = field(
        default=True,
        metadata={"help": "Compress the quantization statistics through double quantization."}
    )
    quant_type: str = field(
        default="nf4",
        metadata={"help": "Quantization data type to use. Should be one of `fp4` or `nf4`."}
    )
    bits: int = field(
        default=16,
        metadata={"help": "How many bits to use."}
    )
    lora_enable: bool = False
    lora_r: int = 64
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    lora_weight_path: str = ""
    lora_bias: str = "none"


def preprocess_plain_new(
    sources: Sequence[str],
    timestamps: Sequence[List[float]],
    duration:,
    vocab_size:
    tokenizer:
    modal_token: str
):



def preprocess_plain(
    sources: Sequence[str],
    tokenizer: transformers.PreTrainedTokenizer,
    modal_token: str = None,
    llm_name_idx: int = 2 # Gemini -> need to fix
) -> Dict:

    conversations = []
    input_ids = []
    targets = []
    for source in sources:
        # time token


        # 1. apply chat template for input conversation
        assert len(source) == 2
        assert modal_token in source[0]['value']
        message = [
            {'role': 'user', 'content': modal_token},
            {'role': 'assistant', 'content': source[1]['value']}
        ]
        conversation = " ".join([sentence['value'] for sentence in source])

        input_id = tokenizer_multimodal_token(conversation, tokenizer, modal_token, return_tensors='pt')
        target = copy.deepcopy(input_id)
        target[input_id == MODAL_INDEX_MAP[modal_token]] = IGNORE_INDEX

        input_ids.append(input_id)
        targets.append(target)

    return dict(input_ids=input_ids, labels=targets)


def preprocess(
    sources: Sequence[str],
    tokenizer: transformers.PreTrainedTokenizer,
    modal_token: str = None,
) -> Dict:
    roles = {"human": "user", "gpt": "assistant", "Gemini": "assistant", "Qwen": "assistant", "chatgpt": "assistant"}

    # Apply prompt templates
    conversations = []
    input_ids = []
    targets = []
    for i, source in enumerate(sources):
        if roles[source[0]["from"]] != "user":
            # Skip the first one if it is not from human
            source = source[1:]

        message = [{'role': roles[sentence['from']], 'content': sentence['value']} for sentence in source]
        conversation = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)
        input_ids.append(tokenizer_multimodal_token(conversation, tokenizer, modal_token, return_tensors='pt'))
        targets.append(copy.deepcopy(input_ids[-1]))

        assert len(source) % 2 == 0, f"Invalid conversation length {len(source)}."

        cur = 0
        message = []
        for idx, sentence in enumerate(source):
            if idx % 2 == 1:
                tmp_message = [
                    {'role': roles[source[idx-1]['from']], 'content': source[idx-1]['value']}, 
                    {'role': roles[sentence['from']], 'content': sentence['value']}
                ]

                instruction = tokenizer.apply_chat_template(message + tmp_message[:1], tokenize=False, add_generation_prompt=True)
                conversation = tokenizer.apply_chat_template(message + tmp_message, tokenize=False, add_generation_prompt=False)

                instruction_len = len(tokenizer_multimodal_token(instruction, tokenizer, modal_token, return_tensors='pt'))
                conversation_len = len(tokenizer_multimodal_token(conversation, tokenizer, modal_token, return_tensors='pt'))

                targets[-1][cur:instruction_len] = IGNORE_INDEX

                cur = conversation_len
                message += tmp_message

    return dict(input_ids=input_ids, labels=targets)


def preprocess_multimodal(
    sources: Sequence[str],
    data_args: DataArguments,
    modal_token: str = None,
    # llm_name_idx: int = 2 # Gemini -> need to fix
) -> Dict:
    is_multimodal = data_args.is_multimodal
    if not is_multimodal:
        return sources
    
    assert modal_token in MODAL_INDEX_MAP, f"Unsupported modal token {modal_token}."
    for source in sources:
        for event in source:
            for sentence in event:
                if sentence['value']:
                    if modal_token in sentence['value']:
                        sentence['value'] = sentence['value'].replace(modal_token, '').strip()
                        sentence['value'] = modal_token + '\n' + sentence['value']
                        sentence['value'] = sentence['value'].strip()
                    replace_token = modal_token
                    # TODO: fix this for multimedia, e.g., <video>, <audio>, etc.
                    sentence["value"] = sentence["value"].replace(modal_token, replace_token)
    return sources


def inject_time_tokens(
    sources: Dict,
    num_bins: int = 100,
    duration: float = None
) -> Dict:
    """
    Inject time tokens into the conversation based on timestamp information.
    This modifies the conversation in-place.
    """
    if 'timestamp' in sources:
        timestamps = sources["timestamp"]
        conversations = sources["conversation"]
        
        # Ensure we have timestamps for each event
        if len(conversations) == len(timestamps):
            for i, event_conv in enumerate(conversations):
                ts = timestamps[i]
                # Handle potential nesting if timestamp is list of lists
                start = ts[0]
                end = ts[1]
                
                time_tokens = timestamp_to_time_token(start, end, duration, num_bins=num_bins)
                time_str = "".join(time_tokens)
                
                # Find the assistant message to prepend/append timestamp
                found_assistant = False
                for msg in event_conv:
                    # Check for assistant/gpt role. Assuming 'gpt' or 'assistant' or specific LLMs
                    if msg['from'] in ['gpt', 'chatgpt', 'assistant', 'Gemini', 'Qwen']:
                        # Vid2Seq format: <time_start><time_end> description
                        msg['value'] = f"{time_str} {msg['value']}"
                        found_assistant = True
                        break # Only modify the first assistant response
                
                if not found_assistant:
                    # Fallback: if no assistant message, maybe append to user? 
                    # Or just ignore/print warning.
                    pass
    return sources

def preprocess_timestamps(timestamps: List[List[float]], duration: float, vocab_size: int) -> torch.Tensor:
    starts, ends = [], []

    print('timestamp', timestamps)
    print('duration', duration)
    print('vocab_size', vocab_size)

    for timestamp in timestamps:
        starts.append(timestamp[0])
        ends.append(timestamp[1])

    print('starts', starts)
    print('ends', ends)

    start_tensor = torch.tensor(starts, dtype=torch.float32)
    end_tensor = torch.tensor(ends, dtype=torch.float32)

    timestamp_tensor = timestampify_pt(start_tensor, end_tensor, duration, vocabulary_size=vocab_size)
    return timestamp_tensor

class LazySupervisedDataset(Dataset):
    """Dataset for supervised fine-tuning."""

    def __init__(self, data_path: str,
                 tokenizer: transformers.PreTrainedTokenizer,
                 data_args: DataArguments):
        super(LazySupervisedDataset, self).__init__()
        list_data_dict = json.load(open(data_path, "r"))
        list_data_dict = self.llm_response_selection(list_data_dict, data_args.llm_response_selector)
        print(len(list_data_dict), "samples loaded from", data_path)
        
        rank0_print("Formatting inputs...Skip in lazy mode")
        self.tokenizer = tokenizer
        self.list_data_dict = list_data_dict
        self.data_args = data_args

        self.vocab_size = len(tokenizer)

    def __len__(self):
        return len(self.list_data_dict)

    @property
    def lengths(self):
        length_list = []
        for sample in self.list_data_dict:
            img_tokens = 576 if 'image' in sample else 0
            length_list.append(sum(len(conv['value'].split()) for conv in sample['conversations']) + img_tokens)
        return length_list


    # input nhiễu mẫu của video_data
    def llm_response_selection(self, video_data: Dict, llm_name: str) -> List[str]:
        selected_video_data = copy.deepcopy(video_data)

        for sample in selected_video_data:
            new_conversations = []

            for event_conversation in sample['conversation']:
                new_message = [
                    # {
                    #     'from': 'human',
                    #     'value': "Describe the video by following guidelines: you should give a paragraph with maximum 75 words; focus on the most obvious feature of the main objects <region>, <region> and <region>; infer the behavior of the object (feeding, resting, breathing, social interactions, defense); and describe the background in about 10 words. Focus on fish, reefs, aquatic plants, wrecks, human divers, and sea floor. Omit the words 'underwater' and 'shows' in the video\n<video>."
                    # },
                    {
                        'from': llm_name,
                        'value': next(
                            message['value']
                            for message in event_conversation
                            if message['from'] == llm_name
                        )
                    }
                ]

                new_conversations.append(new_message)

            sample['conversation'] = new_conversations

        return selected_video_data

    @property
    def modality_lengths(self):
        length_list = []
        for sample in self.list_data_dict:
            cur_len = sum(len(conv['value'].split()) for conv in sample['conversations'])
            cur_len = cur_len if 'image' in sample else -cur_len
            length_list.append(cur_len)
        return length_list

    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        sources = self.list_data_dict[i]
        # =----- not need now: ignore
        # image_processor = self.data_args.image_processor
        video_processor = self.data_args.video_processor

        num_frames = NUM_FRAMES if self.data_args.num_frames is None else self.data_args.num_frames
 
        if isinstance(i, int):
            sources = self.list_data_dict[i]
            sources = [sources]
        assert len(sources) == 1, "Don't know why it is wrapped to a list"  # FIXME
        ann_indices = []
        frame_nums = 1

        # if jmage ignore
        if 'image' in sources[0]:
            # print(sources[0]['image'])
            image_file = self.list_data_dict[i]['image']
            image_file = os.path.join(self.data_args.data_folder, image_file)

            try:
                image, height, width = process_image(image_file, image_processor, self.data_args.image_aspect_ratio)
                image = image[0]
            except Exception as e:
                traceback.print_exc()
                backup_idx = random.randint(0, len(self.list_data_dict)-1)
                print(f"Encounted error when reading image {image_file}, use {backup_idx}-th example instead!!!")
                return self.__getitem__(backup_idx)
            # place <image> tag to question head.
            modal_token = "<image>"
            conversations = preprocess_multimodal(copy.deepcopy([e["conversation"] for e in conversations]), self.data_args, modal_token)
        
        # main proccess
        elif 'video' in sources[0]:
            # video path proccessing            
            video_file = self.list_data_dict[i]['video']
            video_file = os.path.join(self.data_args.data_folder, video_file)
            print("File exists:", os.path.exists(video_file))
            all_frames = set()
            
            # # frame proccessing: uisng later
            try: 
                if False and 'annotation' in sources[0]:
                    for ann in sources[0]['annotation']:
                        all_frames.update(list(ann.keys()))
                    all_frames = list(all_frames)
                    frame_nums = len(all_frames)
                    for ann in sources[0]['annotation']:
                        frame_list = list(ann.keys())
                        indices = []
                        for frame in frame_list:
                            indices.append(all_frames.index(frame))
                        ann_indices.append(indices)
                else: 
                    all_frames.add(0)
                    ann_indices.append([0])

                all_frames = [int(f) for f in all_frames]
                video, frame, height, width, _ = process_video(video_file, video_processor, aspect_ratio=self.data_args.image_aspect_ratio, num_frames=num_frames, frame_idx=all_frames) #frame [1,3,336,336]
            except Exception as e:
                traceback.print_exc()
                backup_idx = random.randint(0, len(self.list_data_dict)-1)
                print(f"Encounted error when reading video {video_file}, use {backup_idx}-th example instead!!!")
                return self.__getitem__(backup_idx)

            # replement frame proccessing
            video, frame, height, width, duration = process_video(video_file, video_processor, aspect_ratio=self.data_args.image_aspect_ratio, num_frames=num_frames, frame_idx=all_frames) #frame [1,3,336,336]

            # place <video> tag to question head.
            modal_token = "<video>"
            
            # NOTE: We must update sources in-place or assign back, because preprocess() uses sources.
            # Using deepcopy without assignment means sources is not updated with <video> token.
            sources[0]["conversation"] = preprocess_multimodal(
                copy.deepcopy([sources[0]["conversation"]]), 
                self.data_args, 
                modal_token
            )[0]
            
            # Inject time tokens into conversation
            sources[0] = inject_time_tokens(sources[0], num_bins=NUM_TIME_BINS, duration=duration)
            conversations = copy.deepcopy([e["conversation"] for e in sources])

            
            # time_stamps tensor is no longer needed since we tokenized them into the text
            # time_stamps = preprocess_timestamps(...) 
            # print("Time stamps: ", time_stamps)
            # raise
        else:
            modal_token = None
            conversations = copy.deepcopy([e["conversation"] for e in sources])
        
        print("Conversations: ", conversations[0])

        # ========== segmentation mask: ignore
        masks = []

        # if 'annotation' in self.list_data_dict[i]:
        #     if 'height' in self.list_data_dict[i]:
        #         h = self.list_data_dict[i]['height']
        #         w = self.list_data_dict[i]['width']
        #     else:
        #         h = None
        #         w = None

        #     for anns in self.list_data_dict[i]['annotation']:
        #         for ann_idx in anns.keys():
        #             if anns[ann_idx]['segmentation'] is None:
        #                 mask = np.zeros((height, width))
        #             else:
        #                 mask = annToMask(anns[ann_idx]['segmentation'], h, w)
        #             masks.append(mask)
                    
        #     if 'image' in self.list_data_dict[i]:
        #         ann_indices = [[0]]*len(self.list_data_dict[i]['annotation'])
                
        #     masks = np.array(masks)      
        # else:
        #     masks = np.zeros((1, 336, 336))
            
        # ============ tokenizer proccessing ========
        # Ensure conversations is updated and ready
        if 'conversation' in sources[0]:
             # Merge all events into a single User -> Assistant turn
             # s["conversation"] is [[msg], [msg]] -> Single Conversation
             conversations = []
             for s in sources:
                 events = s["conversation"]
                 # Extract all assistant responses (now with time tokens)
                 # structure of event is [{'from': 'Gemini', 'value': '<t>...'}]
                 
                 # Join all parts with a separator (e.g., space or newline)
                 # Assuming each event has 1 message which is the assistant response
                 full_response_parts = []
                 for event in events:
                     for msg in event:
                         full_response_parts.append(msg['value'])
                 
                 full_response = " ".join(full_response_parts)
                 
                 # Create the single-turn conversation
                 # TODO: Make the system prompt configurable or random? 
                 # For now, using a standard Vid2Seq-like prompt.
                 user_prompt = "Describe the video in detail with timestamps."
                 
                 new_conv = [
                     {'from': 'human', 'value': user_prompt},
                     {'from': 'gpt', 'value': full_response}
                 ]
                 conversations.append(new_conv)
        
        if self.data_args.is_pretraining:
            data_dict = preprocess_plain(conversations, self.tokenizer, modal_token=modal_token)
        else:
            data_dict = preprocess(conversations, self.tokenizer, modal_token=modal_token)

        if isinstance(i, int):
            data_dict = dict(input_ids=data_dict["input_ids"][0], labels=data_dict["labels"][0])

        if 'image' in self.list_data_dict[i]:
            data_dict['image'] = image
            data_dict['frame'] = image.unsqueeze(0)
        elif 'video' in self.list_data_dict[i]:
            data_dict['video'] = video
            data_dict['frame'] = frame ## 暂时第一帧
        elif self.data_args.is_multimodal:
            data_dict['image'] = torch.zeros(3, self.data_args.image_size, self.data_args.image_size)
            data_dict['frame'] = torch.zeros(1, 3, self.data_args.image_size, self.data_args.image_size)

        data_dict['frame_nums'] = frame_nums

        data_dict['masks'] = torch.Tensor(masks)
        if len(ann_indices)==0:
            ann_indices = [[0]]
        data_dict['ann_indices'] = ann_indices

        return data_dict


@dataclass
class DataCollatorForSupervisedDataset(object):
    """Collate examples for supervised fine-tuning."""

    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        input_ids, labels, masks, frame, ann_indices, frame_nums = tuple([instance[key] for instance in instances]
                                  for key in ("input_ids", "labels", "masks", "frame", "ann_indices", "frame_nums"))
        cur_frame_num = 0
        for i, num in enumerate(frame_nums):
            ann_indices[i] = [[x + cur_frame_num for x in sublist] for sublist in ann_indices[i]] 
            cur_frame_num += int(num)

        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids,
            batch_first=True,
            padding_value=self.tokenizer.pad_token_id)
        labels = torch.nn.utils.rnn.pad_sequence(labels,
                                                 batch_first=True,
                                                 padding_value=IGNORE_INDEX)
        input_ids = input_ids[:, :self.tokenizer.model_max_length]
        labels = labels[:, :self.tokenizer.model_max_length]
        batch = dict(
            input_ids=input_ids,
            labels=labels,
            masks=masks,
            frame=frame,
            ann_indices=ann_indices,
            frame_nums=frame_nums,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )
        
        batch['images'] = []
        for instance in instances:
            for modal_token in MODAL_INDEX_MAP.keys():
                modal_token = modal_token.lower()
                # MODAL_TOKEN shape like: <image>, <video>, ...
                modal_name = re.findall(f'[<](.*)[>]', modal_token)
                assert len(modal_name) == 1
                modal_name = modal_name[0]
                if modal_name in instance:
                    batch['images'].append((instance[modal_name], modal_name))

        return batch


def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,
                                data_args) -> Dict:
    """Make dataset and collator for supervised fine-tuning."""
    train_dataset = LazySupervisedDataset(
        tokenizer=tokenizer,
        data_path=data_args.data_path,
        data_args=data_args
    )
    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)
    return dict(train_dataset=train_dataset,
                eval_dataset=None,
                data_collator=data_collator)


def train(attn_implementation=None):
    global local_rank
    set_seed(42)

    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    local_rank = training_args.local_rank
    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))

    bnb_model_from_pretrained_args = {}
    if training_args.bits in [4, 8]:
        from transformers import BitsAndBytesConfig
        bnb_model_from_pretrained_args.update(dict(
            # device_map={"": training_args.device},
            # BUG: High version transformers report error: 
            # ValueError: You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time
            # load_in_4bit=training_args.bits == 4,
            # load_in_8bit=training_args.bits == 8,
            quantization_config=BitsAndBytesConfig(
                load_in_4bit=training_args.bits == 4,
                load_in_8bit=training_args.bits == 8,
                llm_int8_skip_modules=["mm_projector"],
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=training_args.double_quant,
                bnb_4bit_quant_type=training_args.quant_type, # {'fp4', 'nf4'}
                bnb_4bit_quant_storage=compute_dtype,
            )
        ))

    config = VLLMConfigs[model_args.model_type].from_pretrained(model_args.model_path, trust_remote_code=True)
    config._attn_implementation = attn_implementation

    if model_args.vision_tower is not None:
        model = VLLMs[model_args.model_type].from_pretrained(
            model_args.model_path,
            config=config,
            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),
            do_sample=True,
            **bnb_model_from_pretrained_args
        )
        if 'mixtral' in model_args.model_type:
            import deepspeed
            deepspeed.utils.set_z3_leaf_modules(model, [MixtralSparseMoeBlock])
    else:
        model = transformers.LlamaForCausalLM.from_pretrained(
            model_args.model_path,
            config=config,
            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),
            do_sample=True,
            **bnb_model_from_pretrained_args
        )
    model.config.use_cache = False

    if model_args.freeze_backbone:
        model.model.requires_grad_(False)

    if training_args.bits in [4, 8]:
        from peft import prepare_model_for_kbit_training
        model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)

    if training_args.gradient_checkpointing:
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
        else:
            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)
            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

    if training_args.lora_enable:
        from peft import LoraConfig, get_peft_model
        lora_config = LoraConfig(
            r=training_args.lora_r,
            lora_alpha=training_args.lora_alpha,
            target_modules=find_all_linear_names(model),
            lora_dropout=training_args.lora_dropout,
            bias=training_args.lora_bias,
            task_type="CAUSAL_LM",
        )
        if training_args.bits == 16:
            if training_args.bf16:
                model.to(torch.bfloat16)
            if training_args.fp16:
                model.to(torch.float16)
        rank0_print("Adding LoRA adapters...")
        model = get_peft_model(model, lora_config)


    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_path,
        model_max_length=training_args.model_max_length,
        padding_side="right",
        use_fast=True,
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.unk_token

    # Add time tokens to tokenizer
    num_time_tokens = NUM_TIME_BINS
    time_tokens = [f"<time_{i}>" for i in range(num_time_tokens)]
    num_new_tokens = tokenizer.add_tokens(time_tokens)
    if num_new_tokens > 0:
        model.resize_token_embeddings(len(tokenizer))
        rank0_print(f"Added {num_new_tokens} new time tokens to tokenizer and resized model embeddings.")


    if model_args.vision_tower is not None:
        # initialize vision encoder + multi-modal projector
        model.get_model().initialize_vision_modules(model_args=model_args, fsdp=training_args.fsdp)

        vision_tower = model.get_vision_tower()
        vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)

        data_args.image_size = vision_tower.image_size

        data_args.image_processor = vision_tower.image_processor
        data_args.video_processor = vision_tower.video_processor if hasattr(vision_tower, "video_processor") else vision_tower.image_processor

        data_args.is_multimodal = True

        model.config.image_aspect_ratio = data_args.image_aspect_ratio
        model.config.tokenizer_padding_side = tokenizer.padding_side
        model.config.tokenizer_model_max_length = tokenizer.model_max_length

        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter
        model.config.tune_region_encoder = training_args.tune_region_encoder = model_args.tune_region_encoder
    
        if model_args.tune_mm_mlp_adapter:
            model.requires_grad_(False)
            for p in model.get_model().mm_projector.parameters():
                p.requires_grad = True
        
        if model_args.tune_region_encoder:
            model.requires_grad_(False)
            for p in model.get_model().region_encoder.parameters():
                p.requires_grad = True
                
        if model_args.tune_mm_mlp_adapter:
            data_args.is_pretraining = True
        else:
            data_args.is_pretraining = False

        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter
        if training_args.freeze_mm_mlp_adapter:
            for p in model.get_model().mm_projector.parameters():
                p.requires_grad = False

        if training_args.bits in [4, 8]:
            model.get_model().mm_projector.to(dtype=compute_dtype, device=training_args.device)

        model.config.mm_projector_lr = training_args.mm_projector_lr
        model.config.num_frames = NUM_FRAMES if data_args.num_frames is None else data_args.num_frames
        
        model.initialize_MM_tokenizer(tokenizer=tokenizer)

    if training_args.bits in [4, 8]:
        from peft.tuners.lora import LoraLayer
        for name, module in model.named_modules():
            if isinstance(module, LoraLayer):
                if training_args.bf16:
                    module = module.to(torch.bfloat16)
            if 'norm' in name:
                module = module.to(torch.float32)
            if 'lm_head' in name or 'embed_tokens' in name:
                if hasattr(module, 'weight'):
                    if training_args.bf16 and module.weight.dtype == torch.float32:
                        module = module.to(torch.bfloat16)

    print("Current model:", model)
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
    # select a Trainer
    trainer = VideoReferTrainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)

    if list(pathlib.Path(training_args.output_dir).glob("checkpoint-*")):
        trainer.train(resume_from_checkpoint=True)
    else:
        trainer.train()
    trainer.save_state()

    model.config.use_cache = True

    if training_args.lora_enable:
        state_dict = get_peft_state_maybe_zero_3(model.named_parameters(), training_args.lora_bias)
        non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(model.named_parameters())
        if training_args.local_rank == 0 or training_args.local_rank == -1:
            model.config.save_pretrained(training_args.output_dir)
            model.save_pretrained(training_args.output_dir, state_dict=state_dict)
            torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, 'non_lora_trainables.bin'))
    else:
        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)


if __name__ == "__main__":
    train("flash_attention_2")
